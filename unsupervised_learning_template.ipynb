{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Nikki Satmaka - Batch 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Dataset is taken from [Google Cloud Platform BigQuery](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=ml_datasets&t=credit_card_default&page=table)\n",
    "\n",
    "Context:\n",
    "\n",
    "This dataset contains \n",
    "\n",
    "### Objective\n",
    "\n",
    "- pass\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "- pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "We first need to query our data from GCP's BiqQuery using this code\n",
    "\n",
    "```SQL\n",
    "SELECT *\n",
    "FROM table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Handling outlier\n",
    "from feature_engine.outliers import OutlierTrimmer\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.outliers import ArbitraryOutlierCapper\n",
    "\n",
    "# For Feature Engineering\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For Handling Missing Values\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import RandomSampleImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "\n",
    "# For Data Preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# For Classification Problems\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split Dataset and Standarize the Datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Evaluate Classification Models\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "sns.set_theme(style='darkgrid', palette='deep')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique(data, col_type='both'):\n",
    "    \"\"\"\n",
    "    Count the number of unique values in each features for 'numeric', 'categorical', or 'both'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "\n",
    "    col_type : str\n",
    "        The type of the column to filter. Either 'number', 'object', or 'both'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Number of unique values of each features\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the column type is valid\n",
    "    if col_type not in ('number', 'object', 'both'):\n",
    "        raise ValueError('col_type must be either \"number\", \"object\", or \"both\"')\n",
    "\n",
    "    # create a list if the column type is 'both'\n",
    "    if col_type == 'both':\n",
    "        col_type = ['number', 'object']\n",
    "\n",
    "    # get the number of unique values in each column\n",
    "    data_unique_count = pd.DataFrame.from_records(\n",
    "        [(col, data[col].nunique()) for col in data.select_dtypes(include=col_type).columns],\n",
    "        columns=['feats', 'num_unique']\n",
    "    )\n",
    "\n",
    "    return data_unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QTywsfA5QXOj"
   },
   "outputs": [],
   "source": [
    "def find_normal_boundaries(data, variable):\n",
    "    \"\"\"\n",
    "    Calculate the boundaries outside which sit the outliers for a Gaussian distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "\n",
    "    variable : string\n",
    "        The feature of the DataFrame in which to the calculation will be performed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    upper_boundary : float\n",
    "        The computed upper boundary of the data\n",
    "\n",
    "    lower_boundary : float\n",
    "        The computed lower boundary of the data\n",
    "    \"\"\"\n",
    "\n",
    "    upper_boundary = data[variable].mean() + 3 * data[variable].std()\n",
    "    lower_boundary = data[variable].mean() - 3 * data[variable].std()\n",
    "\n",
    "    return upper_boundary, lower_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zuSeqy1BQXOm"
   },
   "outputs": [],
   "source": [
    "def find_skewed_boundaries(data, variable, distance):\n",
    "    \"\"\"\n",
    "    Calculate the boundaries outside which sit the outliers for skewed distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "\n",
    "    variable : string\n",
    "        The feature of the DataFrame in which to the calculation will be performed\n",
    "\n",
    "    distance : float\n",
    "        The distance multiplier of IQR to calculate the boundaries\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    upper_boundary : float\n",
    "        The computed upper boundary of the data\n",
    "\n",
    "    lower_boundary : float\n",
    "        The computed lower boundary of the data\n",
    "    \"\"\"\n",
    "\n",
    "    IQR = data[variable].quantile(0.75) - data[variable].quantile(0.25)\n",
    "\n",
    "    upper_boundary = data[variable].quantile(0.75) + (IQR * distance)\n",
    "    lower_boundary = data[variable].quantile(0.25) - (IQR * distance)\n",
    "\n",
    "    return upper_boundary, lower_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dist(data):\n",
    "    \"\"\"\n",
    "    Check the Skewness and Distribution for each features in a dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Skewness and distribution types of each features\n",
    "    \"\"\"\n",
    "\n",
    "    # create a DataFrame containing the features of the dataset and their respective skewness\n",
    "    data_skewness = pd.DataFrame(data.skew(), columns=['skew']).reset_index()\n",
    "\n",
    "    # reset the index and make the features columns\n",
    "    data_skewness = data_skewness.rename(columns={'index': 'feats'})\n",
    "\n",
    "    # create a new column to describe whether the feature in the dataset is normal or skewed\n",
    "    data_skewness['dist'] = np.where(\n",
    "        (data_skewness['skew'] > -0.5) & (data_skewness['skew'] < 0.5),\n",
    "        'normal',\n",
    "        'skewed'\n",
    "    )\n",
    "\n",
    "    return data_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outlier(data, distance=1.5):\n",
    "    \"\"\"\n",
    "    Check the outlier info for each features in a dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "\n",
    "    distance : float\n",
    "        The distance multiplier of IQR to calculate the boundaries for skewed distributions. It's either 1.5 or 3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Outlier infos such as upper and lower boundary, and also the number of outliers for each features\n",
    "    \"\"\"\n",
    "\n",
    "    if distance not in (1.5, 3):\n",
    "        raise ValueError('Parameter distance only accepts numeric value of either 1.5 or 3')\n",
    "\n",
    "    data_skewness = check_dist(data)\n",
    "\n",
    "    # create a dictionary to store the outlier infos\n",
    "    data_outlier = {\n",
    "        'feats': [],\n",
    "        'upper_bound': [],\n",
    "        'lower_bound': [],\n",
    "        'tot_right_tail': [],\n",
    "        'tot_left_tail': [],\n",
    "        'tot_right_tail_pct': [],\n",
    "        'tot_left_tail_pct': [],\n",
    "        'tot_outlier': [],\n",
    "        'tot_outlier_pct': [],\n",
    "    }\n",
    "\n",
    "    # loop over each row in the `skewness` DataFrame\n",
    "    # calculate each features upper and lower boundaries and the outlier percentage\n",
    "    for row in data_skewness.index:\n",
    "        col = data_skewness.iloc[row]['feats']\n",
    "\n",
    "        if data_skewness.iloc[row]['dist'] == 'normal':\n",
    "            upper_bound, lower_bound = find_normal_boundaries(data, col)\n",
    "        else:\n",
    "            upper_bound, lower_bound = find_skewed_boundaries(data, col, distance)\n",
    "\n",
    "        tot_right_tail = len(data[data[col] > upper_bound])\n",
    "        tot_left_tail = len(data[data[col] < lower_bound])\n",
    "        tot_right_tail_pct = tot_right_tail / len(data) * 100\n",
    "        tot_left_tail_pct = tot_left_tail / len(data) * 100\n",
    "        tot_outlier =  tot_right_tail + tot_left_tail\n",
    "        tot_outlier_pct = tot_right_tail_pct + tot_left_tail_pct\n",
    "\n",
    "        data_outlier['feats'].append(col)\n",
    "        data_outlier['upper_bound'].append(upper_bound)\n",
    "        data_outlier['lower_bound'].append(lower_bound)\n",
    "        data_outlier['tot_right_tail'].append(tot_right_tail)\n",
    "        data_outlier['tot_left_tail'].append(tot_left_tail)\n",
    "        data_outlier['tot_right_tail_pct'].append(tot_right_tail_pct)\n",
    "        data_outlier['tot_left_tail_pct'].append(tot_left_tail_pct)\n",
    "        data_outlier['tot_outlier'].append(tot_outlier)\n",
    "        data_outlier['tot_outlier_pct'].append(tot_outlier_pct)\n",
    "    \n",
    "    data_outlier = pd.DataFrame(data_outlier)\n",
    "\n",
    "    return data_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_summary(data, distance=1.5):\n",
    "    \"\"\"\n",
    "    Check the summary for outlier data, such as distribution and number of outliers for each features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "\n",
    "    distance : float\n",
    "        The distance multiplier of IQR to calculate the boundaries for skewed distributions. It's either 1.5 or 3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Summary of outlier such as distribution and number of outliers for each features\n",
    "    \"\"\"\n",
    "\n",
    "    data_skewness = check_dist(data)\n",
    "    data_outlier = check_outlier(data, distance)\n",
    "\n",
    "    outlier_summary_cols = ['feats', 'skew', 'dist', 'tot_outlier', 'tot_outlier_pct']\n",
    "\n",
    "    data_outlier_summary = pd.merge(data_skewness, data_outlier, on=['feats'])\n",
    "    data_outlier_summary = data_outlier_summary[outlier_summary_cols]\n",
    "\n",
    "    return data_outlier_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HuJda0YTS38t"
   },
   "outputs": [],
   "source": [
    "def impute_na(data, variable, mean_value, median_value):\n",
    "  \"\"\"\n",
    "  Function to Fill Missing Values with Zeroes, Mean, and Median\n",
    "  \"\"\"\n",
    "  data[variable+'_mean'] = data[variable].fillna(mean_value)\n",
    "  data[variable+'_median'] = data[variable].fillna(median_value)\n",
    "  data[variable+'_zero'] = data[variable].fillna(0)\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df_ori = pd.read_csv('data/')\n",
    "df = df_ori.copy()\n",
    "\n",
    "# display the first 5 entries of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the last 5 entries of the data\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are x entries and y columns of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in dataset\n",
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Characteristics of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check basic stats for features with number dtypes\n",
    "df.describe(percentiles=[0.5]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check basic stats for features with object dtypes\n",
    "df.describe(include='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cardinality of each features\n",
    "print(\"Features with low cardinality:\")\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() < 20:\n",
    "        print(col, ':', df[col].nunique(), 'unique values \\n', np.sort(df[col].unique()))\n",
    "        print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cover some basic stats of some numerical features in the train set.\n",
    "- `feature`\n",
    "    - pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Handle cardinality in `feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data looks good and is in accordance with the data design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Dataset Imbalance\n",
    "\n",
    "Check whether the target variable of the dataset is balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for imbalance in target variable\n",
    "df['target'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is imbalance, we need to stratify when splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset\n",
    "\n",
    "We need to split the dataset into inference, train and test sets before we do any EDA.\\\n",
    "We do our EDA on the train set so as to not have any bias towards the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample dataset for inference\n",
    "df_inf = df.sample(10, random_state=42)\n",
    "\n",
    "# remove inference set from original dataset\n",
    "df_train_test = df.drop(df_inf.index).reset_index(drop=True)\n",
    "\n",
    "# reset index for inference set\n",
    "df_inf = df_inf.reset_index(drop=True)\n",
    "\n",
    "print('df_inf Size:', df_inf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test set\n",
    "\n",
    "\n",
    "Since the target variable is imbalanced, we use stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use stratified sampling to ensure that the distribution of the target variable is balanced\n",
    "df_train, df_test = train_test_split(\n",
    "    df_train_test,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=df_train_test['target']\n",
    ")\n",
    "\n",
    "print('df_train Size:', df_train.shape)\n",
    "print('df_test Size:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup the train set that we are gonna perform EDA on\n",
    "df_train_ori = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of dataset\n",
    "\n",
    "The dataset seems to have come from [UCI](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subheading 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subheading 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subheading 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the train set from the backup\n",
    "df_train = df_train_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split between features and target\n",
    "X_train = df_train.drop(['target'], axis=1)\n",
    "y_train = df_train['target'].copy()\n",
    "\n",
    "X_test = df_test.drop(['target'], axis=1)\n",
    "y_test = df_test['target'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing Features\n",
    "\n",
    "Categorize the features based on the variable type of the features and the data it represents\n",
    "- Numeric (Interval): Features which have equally spaced interval between unique values\n",
    "- Categorical (Nominal): Features which have no intrinsic ordering to the unique values\n",
    "- Ordinal: Features which have clear ordering but do not have equally spaced intervals between unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorizing features\n",
    "num_cols = []\n",
    "nom_cols = []\n",
    "ord_cols = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outlier summary only on numerical features\n",
    "outlier_summary(X_train[num_cols], 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Trimming`: if outliers' percentage < 5%\n",
    "2. `Capping`: if outliers' percentage 5% - 15%\n",
    "3. `None`: if outliers' percentage > 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outlier details\n",
    "check_outlier(X_train[num_cols], 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in train set\n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in train target\n",
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in test set\n",
    "X_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in test target\n",
    "y_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! There are no missing values in train nor test features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Correlation Matrix for Numerical Features\n",
    "\n",
    "We look at the Spearman's correlation matrix to find out the relation between features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DytKCFs5Ddt7",
    "outputId": "51beab39-eec8-4664-d174-b96fb87efd5d"
   },
   "outputs": [],
   "source": [
    "# Heatmap Correlation Matrix\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.concat([y_train, X_train[num_cols]], axis=1).corr('spearman'),\n",
    "    annot=True, vmin=0, vmax=1, fmt='.2f', square=True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Heatmap for Numerical Features')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Correlation Matrix for Categorical Features\n",
    "\n",
    "We look at the Spearman's correlation matrix to find out the relation between features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DytKCFs5Ddt7",
    "outputId": "51beab39-eec8-4664-d174-b96fb87efd5d"
   },
   "outputs": [],
   "source": [
    "# Heatmap Correlation Matrix\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.concat([y_train, X_train[nom_cols]], axis=1).corr('spearman'),\n",
    "    annot=True, vmin=0, vmax=1, fmt='.2f', square=True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Heatmap for Categorical Features')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are low spearman correlations amongst features and target\n",
    "- However, since we do not really have any features that stand out, I'm going to use all features as predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Correlation Matrix for Ordinal Features\n",
    "\n",
    "We look at the Spearman's correlation matrix to find out the relation between features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DytKCFs5Ddt7",
    "outputId": "51beab39-eec8-4664-d174-b96fb87efd5d"
   },
   "outputs": [],
   "source": [
    "# Heatmap Correlation Matrix\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.concat([y_train, X_train[ord_cols]], axis=1).corr('spearman'),\n",
    "    annot=True, vmin=0, vmax=1, fmt='.2f', square=True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Heatmap for Ordinal Features')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are quite a moderate correlations between the `pay_x` features and the target, at least higher than any in the numerical and categorical features\n",
    "- However, we can also see that those features tend to be dependent amongst each other\n",
    "- Still, since we do not really have any features that stand out, I'm going to use all features as predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Features\n",
    "\n",
    "These are the predictors we're going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out list of predictors\n",
    "print('Numerical Features:')\n",
    "print(num_cols)\n",
    "print('=' * 50)\n",
    "print('Categorical Features:')\n",
    "print(nom_cols)\n",
    "print('=' * 50)\n",
    "print('Ordinal Features:')\n",
    "print(ord_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "\n",
    "Create a pipeline based on how we would engineer the features, whether to scale or to encode\n",
    "- We will create multiple pipeline for scaling because each features and each models require different procedures in which we handle it\n",
    "- All categorical features will use one hot encoder\n",
    "- Ordinal features which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline for standardization\n",
    "std_pipe = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# create pipeline for min max scaling\n",
    "min_max_pipe = Pipeline([\n",
    "    ('min_max_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# create pipeline for normalizer\n",
    "power_pipe = Pipeline([\n",
    "    ('power_transformer', PowerTransformer())\n",
    "])\n",
    "\n",
    "# create pipeline for robust scaler\n",
    "robust_pipe = Pipeline([\n",
    "    ('robust_scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# create pipeline for categorical features\n",
    "nom_pipe = Pipeline([\n",
    "    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Column Transformer\n",
    "\n",
    "Create a `ColumnTransformer` object based on the pipeline we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column transformer object using standard scaler\n",
    "ct_std = ColumnTransformer([\n",
    "    ('num', std_pipe, num_cols),\n",
    "    ('nom', nom_pipe, nom_cols),\n",
    "    ('ord', 'passthrough', ord_cols)\n",
    "])\n",
    "\n",
    "# create column transformer object using min max scaler\n",
    "ct_mm = ColumnTransformer([\n",
    "    ('num', min_max_pipe, num_cols),\n",
    "    ('nom', nom_pipe, nom_cols),\n",
    "    ('ord', 'passthrough', ord_cols)\n",
    "])\n",
    "\n",
    "# create column transformer object using power transform\n",
    "ct_pt = ColumnTransformer([\n",
    "    ('num', power_pipe, num_cols),\n",
    "    ('nom', nom_pipe, nom_cols),\n",
    "    ('ord', 'passthrough', ord_cols)\n",
    "])\n",
    "\n",
    "# create column transformer object using robust scaler\n",
    "ct_rs = ColumnTransformer([\n",
    "    ('num', robust_pipe, num_cols),\n",
    "    ('nom', nom_pipe, nom_cols),\n",
    "    ('ord', 'passthrough', ord_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Target: Predicting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predictors: The features I'm going to use are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models: The Supervised Learning Algorithms I'm going to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of models\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validating Base Models\n",
    "\n",
    "Since the dataset is imbalance, we are going to evaluate our models based on **F1 Score**\\\n",
    "It's also important that we reduce the number of *False Negatives*, hence a high **Recall** is also important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict to store the cross validation scores\n",
    "cv_results = {\n",
    "    'models': [],\n",
    "    'f1_score_mean': [],\n",
    "    'f1_score_std': [],\n",
    "    'recall_score_mean': [],\n",
    "    'recall_score_std': []\n",
    "} \n",
    "\n",
    "# loop over each each models and perform cross validation\n",
    "for name, model in models.items():\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # get cross validation scores\n",
    "    scores = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        scoring=['f1_weighted', 'recall'],\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    # store the cross validation scores\n",
    "    cv_results['models'].append(name)\n",
    "    cv_results['f1_score_mean'].append(scores['test_f1_weighted'].mean().round(2))\n",
    "    cv_results['f1_score_std'].append(scores['test_f1_weighted'].std().round(4))\n",
    "    cv_results['recall_score_mean'].append(scores['test_recall'].mean().round(2))\n",
    "    cv_results['recall_score_std'].append(scores['test_recall'].std().round(4))\n",
    "\n",
    "# create a dataframe from the dict\n",
    "cv_results_df = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe sorted by f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid search object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# perform grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best parameters\n",
    "\n",
    "# print the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the best estimator to the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this grid search resulted in:\n",
    "- pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train set using the base model\n",
    "\n",
    "# predict test set using the base model\n",
    "\n",
    "# predict train set using the final model\n",
    "\n",
    "# predict test set using the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target names for classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classification report for train set\n",
    "\n",
    "# create classification report for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curve for train set\n",
    "# calculate auc score for train set\n",
    "\n",
    "# plot roc curve for test set\n",
    "# calculate auc score for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classification report for train set\n",
    "\n",
    "# create classification report for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curve for train set\n",
    "# calculate auc score for train set\n",
    "\n",
    "# plot roc curve for test set\n",
    "# calculate auc score for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "1. pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directory for saving model\n",
    "model_dir = 'models'\n",
    "model_name = ''\n",
    "\n",
    "# create directory if it does not exist\n",
    "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model location\n",
    "model_dir = 'models'\n",
    "model_name = ''\n",
    "model_path = Path(model_dir, model_name)\n",
    "\n",
    "# load model\n",
    "final_svm = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display inference set\n",
    "df_inf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# predict inference set using the final model\n",
    "y_pred_inf_svm = final_svm.predict(df_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with predictions\n",
    "df_inf['pred_svm'] = y_pred_inf_svm\n",
    "\n",
    "# display inference set\n",
    "df_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model successfully run on inference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On EDA\n",
    "- pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Modeling\n",
    "- pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implication\n",
    "- pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvement\n",
    "- pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57744ed932a5da4ffb7d5879d9b65170d321805180660db910f737e8fd70cf58"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
